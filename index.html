<!DOCTYPE html>
<!-- saved from url=(0033)https://OFA-Sys.github.io/Demo_FTTS/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Free Text Control Text to Speech</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Free Text Control Text to Speech">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://OFA-Sys.github.io/Demo_FTTS/">
<meta property="og:url" content="https://OFA-Sys.github.io/Demo_FTTS/">
<meta name="twitter:card" content="summary">
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
  </head>
  <body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->
      
      
    </section>

<section class="main-content">
      <h1 id=""><center>Free Text Control Text to Speech</center></h1>

<!-- <center> Jianhong Tu<sup>1</sup>, Zeyu Cui<sup>2</sup>, Xinsheng Wang<sup>3</sup>, Lei Xie<sup>1</sup> </center>
    <center> <sup>1</sup> Northwestern Polytechnical University, China</center>
    <center> <sup>2</sup> Tencent AI Lab, China</center>
    <center> <sup>3</sup> Xiâ€™an Jiaotong University, China</center> -->
	
<h2>0. Contents</h2>
<ol>
  <li><a href="#abstract">Abstract</a></li>
  <li><a href="#transfer">Demos -- Expressive speech synthesis on EmoV-DB</a></li>
  <li><a href="#prediction">Demos -- Zeroshot Expressive speech synthesis on "Little Prince" </a></li>
  <li><a href="#control">Demos -- Zeroshot Expressive speech synthesis on handwriting </a></li>
</ol>

<br><br>
<h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
<p> The goal of expressive text-to-speech is to synthesize natural speech with high expressiveness considering content, prosody, emotion, timbre, etc. 
Thanks to the success of deep learning, neural network-based generative model can synthesize expressive speech by providing explicit labels or implicit controlled conditions in recent years. In this paper, we propose to use generalized free text as context to control speech generation for the first time, and we develop an effective end-to-end framework free-text-controlled TTS (FTTS) with four modules. Experiments show that our framework can generate high-quality expressive speech based on the given free text.
	</p>
	<center><img src='fig/frame2.png'></center>
<br><br>

<h2>2. Demos -- Expressive speech synthesis on EmoV-DB<a name="transfer"></a></h2>
<h3>Corresponding to Section 3.2 in our paper, below lists the samples that are synthesized on EmoV-DB dataset. We compared FTTS with vanillaTTS, LTTS, FTTS-NT.</h3>
<p> .</p>
<table>
<!--  <thead>-->
<!--  </thead>-->
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Controlling Text</strong></td>
      <td style="text-align: left" colspan=3> xxxxxxxxxxxxx</td>
    </tr>
	<tr>
	  <td style="text-align: center" rowspan=1><strong>Content Text</strong></td>
	  <td style="text-align: left" colspan=3> Not at this particular case, Tom, apologized Whittemore. </td>
	</tr>
    <tr>
      <td style="text-align: center" rowspan=1><strong>Label</strong></td>
      <td style="text-align: left" colspan=3> Emotion: Amused, Speaker: Bea</td>
    </tr>
    <tr>
      <td style="text-align: center" rowspan=4><strong>Performance</strong></td>
      <td style="text-align: center" colspan=1> <strong>Ref </strong></center</td>
      <td style="text-align: center" colspan=1>  <strong>valinaTTS </strong></td>
      <td style="text-align: center" colspan=1>  <strong>FTTS </strong></td>
    </tr>
    <tr>
      <td style="text-align: left"><audio src="samples/test/ref/bea_bea_amused_1-15_0002.wav" controls="" preload=""></audio></td>
      <td style="text-align: left"><audio src="samples/test/valinaTTS/bea_bea_amused_1-15_0002.wav" controls="" preload=""></audio></td>
      <td style="text-align: left"><audio src="samples/test/FTTS/bea_bea_amused_1-15_0002.wav" controls="" preload=""></audio></td>
    </tr>
    <tr>
      <td style="text-align: center" colspan=1><strong> LTTS</strong> </td>
      <td style="text-align: center" colspan=1> <strong>FTTS_NT </strong></td>
    </tr>
    <tr>
      <td style="text-align: left"><audio src="samples/test/LTTS/bea_bea_amused_1-15_0002.wav" controls="" preload=""></audio></td>
      <td style="text-align: left"><audio src="samples/test/FTTS_NT/bea_bea_amused_1-15_0002.wav" controls="" preload=""></audio></td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<p><b>Short summary:</b> Experiment result...</p>


<br><br>
<h2>3. Demos -- Zeroshot Emotional speech synthesis on novel <a name="prediction"></a></h2>
<!--<h3>Corresponding to Section 5.2 in our paper, below lists the samples that are synthesized for evaluations on emotion prediction from input text only. We compared MsEmoTTS (proposed) with the TPSE-GST model.</h3>-->

<p><b>Short summary:</b> ......</p>

<br><br>
<h2>4. Demos -- Zeroshot Emotional speech synthesis on handwriting context <a name="control"></a></h2>


<p><b>Short summary:</b> The .....</p>


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub Pages</a>.</span>
      </footer>
    </section>
</body></html>
