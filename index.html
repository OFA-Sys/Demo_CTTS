<!DOCTYPE html>
<!-- saved from url=(0033)https://OFA-Sys.github.io/Demo_CTTS/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Semantic Context-controlled Text-to-Speech</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Semantic Context-controlled Text-to-Speech">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://OFA-Sys.github.io/Demo_CTTS/">
<meta property="og:url" content="https://OFA-Sys.github.io/Demo_CTTS/">
<meta name="twitter:card" content="summary">
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
  </head>
  <body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->
      
      
    </section>

<section class="main-content">
      <h1 id=""><center>Semantic Context-controlled Text-to-Speech</center></h1>

    <center> Jianhong Tu<sup>1</sup>, Zeyu Cui<sup>2</sup>, Xiaohuan Zhou<sup>2</sup>, Siqi Zheng<sup>2</sup> , Chang Zhou<sup>2</sup></center>
    <center> <sup>1</sup> Renming University, China</center>
    <center> <sup>2</sup> DAMO Academy, Alibaba Group, China</center>

<h2>0. Contents</h2>
<ol>
  <li><a href="#abstract">Abstract</a></li>
  <li><a href="#transfer">Demos -- Expressive speech synthesis on EmoV-DB</a></li>
  <li><a href="#prediction">Demos -- Zeroshot Expressive speech synthesis on "Little Prince" </a></li>
  <li><a href="#control">Demos -- Zeroshot Expressive speech synthesis on handwriting </a></li>
</ol>

<br><br>
<h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
<p> The goal of expressive Text-to-speech (TTS) is to synthesize natural speech with high expressiveness given content, prosody, emotion, timbre, etc.
  Previous works use reference speeches or explicit labels to control TTS. However, reference speeches do not always exist, and explicit labels could merely cover few aspects of speaker's emotion, personality or identity.
  In this paper, we, for the first time,  introduce a new task setting, \textbf{semantic context-controlled TTS} (CTTS), directly learning the overall information from contextual text and generate speech accordingly.
  To achieve this task, we reorganize a synthetic dataset and develop an effective end-to-end framework. Experiments show that our framework can generate high-quality expressive speech based on the given context both in synthetic dataset and real-world scenario.	</p>
	<center><img src='fig/frame2.png' style="width: 600px;"></center>
<br><br>

<h2>2. Demos -- Expressive speech synthesis on EmoV-DB<a name="transfer"></a></h2>
<h3>Corresponding to Section 3.2 in our paper, below lists the samples that are synthesized on EmoV-DB dataset. We compared FTTS with vanillaTTS, LTTS, FTTS-NT.</h3>
<p> .</p>
<table>
<!--  <thead>-->
<!--  </thead>-->
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Controlling Text</strong></td>
      <td style="text-align: left" colspan=5> xxxxxxxxxxxxx</td>
    </tr>
	<tr>
	  <td style="text-align: center" rowspan=1><strong>Content Text</strong></td>
	  <td style="text-align: left" colspan=5> Not at this particular case, Tom, apologized Whittemore. </td>
	</tr>
    <tr>
      <td style="text-align: center" rowspan=1><strong>Label</strong></td>
      <td style="text-align: left" colspan=5> Emotion: Amused, Speaker: Bea</td>
    </tr>
    <tr>
      <td style="text-align: center" rowspan=2><strong>Performance</strong></td>
      <td style="text-align: center" colspan=1> <strong>Ref </strong></center</td>
      <td style="text-align: center" colspan=1>  <strong>M-TTS </strong></td>
      <td style="text-align: center" colspan=1>  <strong>M-CTTS </strong></td>
      <td style="text-align: center" colspan=1><strong> M-LTTS</strong> </td>
      <td style="text-align: center" colspan=1> <strong>M-CTTS-NT </strong></td>
    </tr>
    <tr>
      <td style="text-align: left"><audio src="samples/test/ref/bea_bea_amused_1-15_0002.wav" style="width: 150px;" controls="" preload=""></audio></td>
      <td style="text-align: left"><audio src="samples/test/M-TTS/bea_bea_amused_1-15_0002.wav" style="width: 150px;" controls="" preload=""></audio></td>
      <td style="text-align: left"><audio src="samples/test/M-CTTS/bea_bea_amused_1-15_0002.wav" style="width: 150px;" controls="" preload=""></audio></td>
      <td style="text-align: left"><audio src="samples/test/M-LTTS/bea_bea_amused_1-15_0002.wav" style="width: 150px;" controls="" preload=""></audio></td>
      <td style="text-align: left"><audio src="samples/test/M-CTTS-NT/bea_bea_amused_1-15_0002.wav" style="width: 150px;" controls="" preload=""></audio></td>
    </tr>

  </tbody>
</table>

<p><b>Short summary:</b> Experiment result...</p>


<br><br>
<h2>3. Demos -- Zeroshot Emotional speech synthesis on novel <a name="prediction"></a></h2>

<p><b>Short summary:</b> ......</p>

<br><br>
<h2>4. Demos -- Zeroshot Emotional speech synthesis on handwriting context <a name="control"></a></h2>


<p><b>Short summary:</b> The .....</p>


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub Pages</a>.</span>
      </footer>
    </section>
</body></html>
